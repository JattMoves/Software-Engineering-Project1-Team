#!/usr/bin/env python3
import json, os, sys, time, subprocess, logging
from typing import List, Dict
from acmecli.logging_setup import configure_logging
from acmecli.url_category import detect_category, model_id_from_hf_url
from acmecli.metrics.ramp_up import compute_ramp_up

TABLE1_ZEROES = {
    "bus_factor": 0.0, "bus_factor_latency": 0,
    "performance_claims": 0.0, "performance_claims_latency": 0,
    "license": 0.0, "license_latency": 0,
    "size_score": {"raspberry_pi": 0.0, "jetson_nano": 0.0, "desktop_pc": 0.0, "aws_server": 0.0},
    "size_score_latency": 0,
    "dataset_and_code_score": 0.0, "dataset_and_code_score_latency": 0,
    "dataset_quality": 0.0, "dataset_quality_latency": 0,
    "code_quality": 0.0, "code_quality_latency": 0,
}

# M2 weights; others 0 for now
WEIGHTS = dict(ramp_up_time=0.20)  # remaining weights realized in Milestone 4

def install() -> int:
    return subprocess.call([sys.executable, "-m", "pip", "install", "--user", "-r", "requirements.txt"])

def run_urls(url_file: str) -> int:
    configure_logging()
    with open(url_file, "r", encoding="utf-8") as f:
        urls = [u.strip() for u in f if u.strip()]

    for url in urls:
        cat = detect_category(url)
        if cat != "MODEL":
            continue  # spec: output for model URLs only

        name = model_id_from_hf_url(url) or url
        # compute Ramp-Up
        r_score, r_ms = compute_ramp_up(name)

        # netscore with current weights (others 0)
        start_ns = time.perf_counter_ns()
        net = WEIGHTS.get("ramp_up_time", 0.0) * r_score
        net_ms = int((time.perf_counter_ns() - start_ns) / 1_000_000)

        record = {
            "name": name,
            "category": "MODEL",
            "net_score": round(float(net), 6),
            "net_score_latency": net_ms,
            "ramp_up_time": round(float(r_score), 6),
            "ramp_up_time_latency": r_ms,
            **TABLE1_ZEROES,
        }
        print(json.dumps(record, ensure_ascii=False))
    return 0

def run_tests() -> int:
    # Run pytest + coverage, then print required summary line
    rc = subprocess.call([sys.executable, "-m", "coverage", "run", "-m", "pytest", "-q"])
    # parse summary
    out = subprocess.check_output([sys.executable, "-m", "coverage", "report"]).decode()
    # naive parse to find percent
    pct = 0
    for line in out.splitlines():
        if "TOTAL" in line:
            pct = int(line.strip().split()[-1].rstrip("%"))
            break
    # count tests from pytest cache (simple fallback)
    try:
        tests_out = subprocess.check_output([sys.executable, "-m", "pytest", "-q"]).decode()
        passed = sum(1 for ln in tests_out.splitlines() if ln.strip().endswith("PASSED"))
        total = passed  # minimal
    except Exception:
        passed = total = 0
    print(f"{passed}/{total} test cases passed. {pct}% line coverage achieved.")
    return rc

def main(argv: List[str]) -> int:
    if len(argv) < 2:
        print("Usage: ./run [install|test|URL_FILE]", file=sys.stderr)
        return 1
    if argv[1] == "install":
        return install()
    if argv[1] == "test":
        return run_tests()
    return run_urls(argv[1])

if __name__ == "__main__":
    sys.exit(main(sys.argv))
