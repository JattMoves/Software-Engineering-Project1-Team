#!/usr/bin/env python3
import json, os, sys, time, subprocess, logging
from typing import List, Dict
from acmecli.logging_setup import configure_logging
from acmecli.url_category import detect_category, model_id_from_hf_url
from acmecli.metrics.ramp_up import compute_ramp_up
# Milestone 4 features (commented out until due)
# from acmecli.metrics.license import license_score
# from acmecli.metrics.code_quality import code_quality_score
# from acmecli.metrics.performance import perf_tracker

TABLE1_ZEROES = {
    "bus_factor": 0.0, "bus_factor_latency": 0,
    "performance_claims": 0.0, "performance_claims_latency": 0,
    "license": 0.0, "license_latency": 0,
    "size_score": {"raspberry_pi": 0.0, "jetson_nano": 0.0, "desktop_pc": 0.0, "aws_server": 0.0},
    "size_score_latency": 0,
    "dataset_and_code_score": 0.0, "dataset_and_code_score_latency": 0,
    "dataset_quality": 0.0, "dataset_quality_latency": 0,
    "code_quality": 0.0, "code_quality_latency": 0,
}

# M2 weights; others 0 for now
WEIGHTS = dict(ramp_up_time=0.20)  # remaining weights realized in Milestone 4

def install() -> int:
    return subprocess.call([sys.executable, "-m", "pip", "install", "--user", "-r", "requirements.txt"])

def run_urls(url_file: str) -> int:
    configure_logging()
    with open(url_file, "r", encoding="utf-8") as f:
        urls = [u.strip() for u in f if u.strip()]

    for url in urls:
        cat = detect_category(url)
        
        if cat == "MODEL":
            name = model_id_from_hf_url(url) or url
            
            # Milestone 3: Only Ramp-Up metric (HF API)
            r_score, r_ms = compute_ramp_up(name)

            # netscore with current weights (others 0)
            start_ns = time.perf_counter_ns()
            net = WEIGHTS.get("ramp_up_time", 0.0) * r_score
            net_ms = int((time.perf_counter_ns() - start_ns) / 1_000_000)

            record = {
                "name": name,
                "category": "MODEL",
                "net_score": round(float(net), 6),
                "net_score_latency": net_ms,
                "ramp_up_time": round(float(r_score), 6),
                "ramp_up_time_latency": r_ms,
                **TABLE1_ZEROES,
            }
            
            # Milestone 4 features (commented out until due)
            # Start performance tracking
            # perf_tracker.start_timer(f"model_{name}")
            # compute License (local analysis metric)
            # l_score, l_ms = license_score(name)
            # compute Code Quality (LLM-based metric)
            # cq_score, cq_ms = code_quality_score(name)
            # End performance tracking
            # total_time = perf_tracker.end_timer(f"model_{name}")
            # record.update({
            #     "license": round(float(l_score), 6),
            #     "license_latency": l_ms,
            #     "code_quality": round(float(cq_score), 6),
            #     "code_quality_latency": cq_ms,
            #     "performance_claims": round(float(total_time), 6),
            #     "performance_claims_latency": int(total_time * 1000),
            # })
        else:
            # For non-MODEL URLs, output basic info
            record = {
                "url": url,
                "category": cat,
            }
        
        print(json.dumps(record, ensure_ascii=False))
    return 0

def run_tests() -> int:
    # First run tests to get count
    result = subprocess.run([sys.executable, "-m", "pytest", "-q", "--tb=no"], 
                          capture_output=True, text=True)
    output = result.stdout
    
    # Count passed tests - look for "passed" in the output
    passed = 0
    for line in output.splitlines():
        if "passed" in line.lower() and "failed" not in line.lower():
            # Extract number from line like "7 passed in 0.67s"
            import re
            match = re.search(r'(\d+)\s+passed', line)
            if match:
                passed = int(match.group(1))
                break
    
    # Count total tests by looking for test files and counting test functions
    import glob
    test_files = glob.glob("tests/test_*.py")
    total = 0
    for test_file in test_files:
        with open(test_file, 'r') as f:
            content = f.read()
            total += content.count("def test_")
    
    # Run pytest + coverage for coverage report
    rc = subprocess.call([sys.executable, "-m", "coverage", "run", "-m", "pytest", "-q"])
    # parse summary
    out = subprocess.check_output([sys.executable, "-m", "coverage", "report"]).decode()
    # naive parse to find percent
    pct = 0
    for line in out.splitlines():
        if "TOTAL" in line:
            pct = int(line.strip().split()[-1].rstrip("%"))
            break
    
    print(f"{passed}/{total} test cases passed. {pct}% line coverage achieved.")
    return rc

def main(argv: List[str]) -> int:
    if len(argv) < 2:
        print("Usage: ./run [install|test|URL_FILE]", file=sys.stderr)
        return 1
    if argv[1] == "install":
        return install()
    if argv[1] == "test":
        return run_tests()
    return run_urls(argv[1])

if __name__ == "__main__":
    sys.exit(main(sys.argv))
